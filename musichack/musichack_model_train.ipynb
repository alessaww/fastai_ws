{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.model import fit\n",
    "from fastai.dataset import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Music modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes from the [IraKorshunova repository](https://github.com/IraKorshunova/folk-rnn/tree/master/data) who has cleaned, parsed and tokenised the [thesession.org dataset](https://github.com/adactio/TheSession-data). We have used the version 3 of this dataset, [allabcwrepeats_parsed_wot](allabcwrepeats_parsed_wot), which has more than 46,000 transcriptions.\n",
    "\n",
    "The **music generation task**\n",
    "\n",
    "We tried to create a *music model*, being inspired by the *language model*, used by Jeremy Howard in [fast.ai course, lesson 4](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson4-imdb.ipynb); where he created a model that can predict the next word in a sentence, in order to finally classify sentiments over specific texts.\n",
    "\n",
    "Because our model first needs to understand the structure of the music files, we decided to use instead of MIDI files, which are matrixes, of 4 columns, text files in [abc format](https://en.wikipedia.org/wiki/ABC_notation).\n",
    "\n",
    "[Ex:](http://abcnotation.com/)\n",
    "```X:1\n",
    "T:Speed the Plough\n",
    "M:4/4\n",
    "C:Trad.\n",
    "K:G\n",
    "|:GABc dedB|dedB dedB|c2ec B2dB|c2A2 A2BA|\n",
    "  GABc dedB|dedB dedB|c2ec B2dB|A2F2 G4:|\n",
    "|:g2gf gdBd|g2f2 e2d2|c2ec B2dB|c2A2 A2df|\n",
    "  g2gf g2Bd|g2f2 e2d2|c2ec B2dB|A2F2 G4:|\n",
    "  ```\n",
    "  \n",
    "![Speed the Plough](https://github.com/alessaww/fastai_ws/blob/master/SpeedThePlough.png?raw=true \"Logo Title Text 1\")\n",
    "\n",
    "There are no good pretrained music models available to download to be used in pytorch, so we need to create our own. \n",
    "\n",
    "We divided the data in 5% for validation and 95% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='data/musichack/thesession/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/  \u001b[01;34mtmp\u001b[0m/  wot_train  wot_valid\r\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M:9/8',\n",
       " 'K:maj',\n",
       " '=G =E =E =E 2 =D =E =D =C | =G =E =E =E =F =G =A =B =c | =G =E =E =E 2 =D =E =D =C | =A =D =D =G =E =C =D 2 =A | =G =E =E =E 2 =D =E =D =C | =G =E =E =E =F =G =A =B =c | =G =E =E =E 2 =D =E =D =C | =A =D =D =G =E =C =D 2 =D | =E =D =E =c 2 =A =B =A =G | =E =D =E =A /2 =B /2 =c =A =B 2 =D | =E =D =E =c 2 =A =B =A =G | =A =D =D =D =E =G =A 2 =D | =E =D =E =c 2 =A =B =A =G | =E =D =E =A /2 =B /2 =c =A =B 2 =B | =G =A =B =c =B =A =B =A =G | =A =D =D =D =E =G =A =B =c |']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = !cat {PATH}wot_valid\n",
    "review[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can analyze text, the text should be *tokenize* first. This refers in the language world, to the process of splitting a sentence into an array of words (or more generally, into an array of tokens).\n",
    "\n",
    "Sturm et all describe in this paper [\"Music transcription modelling and composition using deep learning\"](https://arxiv.org/pdf/1604.08723.pdf)   how he tokenize the music dataset. Here are some tokens used for this dataset:\n",
    "\n",
    "1. meter \"M:9/8\"\n",
    "1. key: \"K:maj\"\n",
    "1. duration \"/2\" and \"2\"\n",
    "1. measure: \":|\" and \"|1\"\n",
    "1. pitch: \"C\" and \"^câ€™\"\n",
    "1. grouping: \"(3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a torchtext field, which describes how to preprocess a piece of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fastai](https://github.com/fastai/fastai) works closely with torchtext. We create a ModelData object for language modeling by taking advantage of `LanguageModelData`, passing it our torchtext field object, and the paths to our training, test, and validation sets. In this case, we don't have a separate test set, so we'll just use `VAL_PATH` for that too.\n",
    "\n",
    "As well as the usual `bs` (batch size) parameter, we also not have `bptt`; this define how many words are processing at a time in each row of the mini-batch. More importantly, it defines how many 'layers' we will backprop through. Making this number higher will increase time and memory requirements, but will improve the model's ability to handle long sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 64,70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = dict(train='wot_train', validation='wot_valid', test='wot_valid')\n",
    "md = LanguageModelData.from_text_files(f'{PATH}', TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building our `ModelData` object, it automatically fills the `TEXT` object with a very important attribute: `TEXT.vocab`. This is a *vocabulary*, which stores which words (or *tokens*) have been seen in the text, and how each word will be mapped to a unique integer id. We'll need to use this information again later, so we save it.\n",
    "\n",
    "*(Technical note: python's standard `Pickle` library can't handle this correctly, so at the top of this notebook we used the `dill` library instead and imported it as `pickle`)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the: # batches; # unique tokens in the vocab; # dataset # tokens in the training set;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1668, 97, 1, 7481251)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the start of the mapping from integer IDs to unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '|', '=c', '=f', '2', '^c', '^g', '^d', '=e', '=g', '=d']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'stoi': 'string to int'\n",
    "TEXT.vocab.stoi['=c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in a `LanguageModelData` object there is only one item in each dataset: all the words of the text joined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M:4/4', 'K:maj', '|:', '=g', '=f', '=e', '=c', '=d', '2', '=g', '=f', '|']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.trn_ds[0].text[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext will handle turning this words into integer IDs for us automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "   44\n",
       "   40\n",
       "   32\n",
       "   26\n",
       "    9\n",
       "   16\n",
       "    4\n",
       "   14\n",
       "    3\n",
       "   26\n",
       "    9\n",
       "    2\n",
       "[torch.cuda.LongTensor of size 12x1 (GPU 0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.numericalize([md.trn_ds[0].text[:12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `LanguageModelData` object will create batches with 64 columns (that's our batch size), and varying sequence lengths of around 80 tokens (that's our `bptt` parameter - *backprop through time*).\n",
    "\n",
    "Each batch also contains the exact same data as labels, but one word later in the text - since we're trying to always predict the next word. The labels are flattened into a 1d array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "    44    39    14  ...     17     7     7\n",
       "    40    36    16  ...     18     6     8\n",
       "    32    15     9  ...      5     4    10\n",
       "        ...          â‹±          ...       \n",
       "     9     8     3  ...     10    27    53\n",
       "    26     2     4  ...      7     2     2\n",
       "     9     4     3  ...     22    25    19\n",
       " [torch.cuda.LongTensor of size 79x64 (GPU 0)], Variable containing:\n",
       "  40\n",
       "  36\n",
       "  16\n",
       "  â‹® \n",
       "   5\n",
       "   3\n",
       "  34\n",
       " [torch.cuda.LongTensor of size 5056 (GPU 0)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 200  # size of each embedding vector\n",
    "nh = 500     # number of hidden activations per layer\n",
    "nl = 3       # number of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers have found that large amounts of *momentum* don't work well with these kinds of *RNN* models, so we create a version of the *Adam* optimizer with less momentum than it's default of `0.9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai uses a variant of the state of the art [AWD LSTM Language Model](https://arxiv.org/abs/1708.02182) developed by Stephen Merity. A key feature of this model is that it provides excellent regularization through [Dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout). There is no simple way known (yet!) to find the best values of the dropout parameters below - you just have to experiment...\n",
    "\n",
    "However, the other parameters (`alpha`, `beta`, and `clip`) shouldn't generally need tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn, em_sz, nh, nl,\n",
    "    dropout=0.05, dropouth=0.1, dropouti=0.05, dropoute=0.02, wdrop=0.2)\n",
    "\n",
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learner.clip=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7579c16576344e0d91228303febd2696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.378355   1.39881   \n",
      "\n",
      "CPU times: user 2min 24s, sys: 1min, total: 3min 24s\n",
      "Wall time: 3min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3988097]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "learner.fit(3e-3, 1, wds=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('adam2_enc_l0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eeeb8cc62734d5c929f6d4cbd6e563e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.289977   1.266527  \n",
      "    1      1.23646    1.238429                                \n",
      "    2      1.194363   1.182311                                \n",
      "    3      1.200287   1.209312                                \n",
      "    4      1.155089   1.154793                                \n",
      "    5      1.12281    1.115545                                \n",
      "    6      1.091392   1.107869                                \n",
      "\n",
      "CPU times: user 16min 51s, sys: 7min 24s, total: 24min 16s\n",
      "Wall time: 23min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1078688]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "learner.fit(3e-3, 3, wds=1e-6, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('adam2_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d2191e15b54a3f83e261c693f500ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.163099   1.174885  \n",
      "    1      1.148263   1.147782                                \n",
      "    2      1.091023   1.114584                                \n",
      "    3      1.068633   1.092056                                \n",
      "    4      1.044709   1.086182                                \n",
      "    5      1.125501   1.145275                                \n",
      "    6      1.091069   1.117792                                \n",
      "    7      1.072576   1.089298                                \n",
      "    8      1.043459   1.067452                                \n",
      "    9      1.027208   1.062771                                \n",
      "    10     1.107522   1.12427                                 \n",
      "    11     1.0635     1.100513                                \n",
      "    12     1.045993   1.067184                                \n",
      "    13     1.022091   1.048181                                \n",
      "    14     1.026159   1.044909                                 \n",
      "    15     1.109666   1.10779                                 \n",
      "    16     1.065488   1.085738                                \n",
      "    17     1.034843   1.055975                                \n",
      "    18     1.02509    1.034824                                 \n",
      "    19     0.984346   1.029295                                 \n",
      "    20     1.065576   1.091504                                \n",
      "    21     1.052549   1.070184                                \n",
      "    22     1.01267    1.042617                                \n",
      "    23     0.983171   1.022165                                 \n",
      "    24     0.978095   1.018434                                 \n",
      "    25     1.069591   1.079691                                \n",
      "    26     1.044595   1.059004                                \n",
      "    27     1.028607   1.034558                                 \n",
      "    28     0.990415   1.010982                                 \n",
      "    29     0.958479   1.007108                                 \n",
      "    30     1.056322   1.06839                                 \n",
      "    31     1.051359   1.047881                                \n",
      "    32     0.996197   1.021536                                 \n",
      "    33     0.987139   0.999699                                 \n",
      "    34     0.956607   0.996976                                 \n",
      "    35     1.032546   1.06161                                 \n",
      "    36     1.034893   1.040702                                \n",
      "    37     0.995762   1.01122                                  \n",
      "    38     0.965638   0.993903                                 \n",
      "    39     0.95599    0.989841                                 \n",
      "    40     1.032552   1.053576                                \n",
      "    41     1.006339   1.031492                                 \n",
      "    42     0.976589   1.007253                                 \n",
      "    43     0.945284   0.98485                                  \n",
      "    44     0.930079   0.981422                                 \n",
      "    45     1.041049   1.050354                                \n",
      "    46     1.007092   1.02676                                  \n",
      "    47     0.971485   0.998525                                 \n",
      "    48     0.9599     0.977767                                 \n",
      "    49     0.917964   0.974897                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.97489655]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 10, wds=1e-6, cycle_len=5, cycle_save_name='adam3_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('adam3_10_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af787bf59ef4fba89350183b31c3306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                 \n",
      "    0      1.006562   1.043507  \n",
      "    1      1.000142   1.037049                                 \n",
      "    2      0.996996   1.027742                                 \n",
      "    3      0.981878   1.012572                                 \n",
      "    4      0.968718   0.995868                                 \n",
      "    5      0.949195   0.981612                                 \n",
      "    6      0.927161   0.9698                                   \n",
      "    7      0.903767   0.961762                                 \n",
      "    8      0.902105   0.955928                                 \n",
      "    9      0.92872    0.954165                                 \n",
      "    10     1.023135   1.030608                                 \n",
      "    11     1.014453   1.026594                                 \n",
      "    12     0.998357   1.014735                                 \n",
      "    13     0.974121   1.00059                                  \n",
      "    14     0.966552   0.984266                                 \n",
      "    15     0.939785   0.970165                                 \n",
      "    16     0.913893   0.95753                                  \n",
      "    17     0.896083   0.946503                                 \n",
      "    18     0.89187    0.94174                                  \n",
      "    19     0.900948   0.939662                                 \n",
      "    20     1.011847   1.0198                                   \n",
      "    21     0.989433   1.014476                                 \n",
      "    22     0.985303   1.002371                                 \n",
      "    23     0.957398   0.98939                                  \n",
      "    24     0.935786   0.976263                                 \n",
      "    25     0.943602   0.958156                                 \n",
      "    26     0.892136   0.944237                                 \n",
      "    27     0.889248   0.931687                                 \n",
      "    28     0.89224    0.92739                                  \n",
      "    29     0.882431   0.927171                                 \n",
      "    30     0.987915   1.00797                                  \n",
      "    31     1.003901   1.004407                                 \n",
      "    32     0.985246   0.993193                                 \n",
      "    33     0.958254   0.981539                                 \n",
      "    34     0.94807    0.963312                                 \n",
      "    35     0.92555    0.946161                                 \n",
      "    36     0.907406   0.931799                                 \n",
      "    37     0.887125   0.925636                                 \n",
      "    38     0.885702   0.920638                                 \n",
      "    39     0.881806   0.916393                                 \n",
      "    40     0.993138   0.999354                                 \n",
      "    41     0.964461   0.99549                                  \n",
      "    42     0.970219   0.979818                                 \n",
      "    43     0.944594   0.967483                                 \n",
      "    44     0.93207    0.951873                                 \n",
      "    45     0.915389   0.939059                                 \n",
      "    46     0.903416   0.920928                                 \n",
      "    47     0.87118    0.910201                                 \n",
      "    48     0.865455   0.90605                                  \n",
      "    49     0.878264   0.903899                                 \n",
      "    50     0.977648   0.99165                                  \n",
      "    51     0.972646   0.987118                                 \n",
      "    52     0.95688    0.976128                                 \n",
      "    53     0.938702   0.960775                                 \n",
      "    54     0.930804   0.941496                                 \n",
      "    55     0.895266   0.928058                                 \n",
      "    56     0.902193   0.910913                                 \n",
      "    57     0.852691   0.902527                                 \n",
      "    58     0.870559   0.895882                                 \n",
      "    59     0.852072   0.894523                                 \n",
      "    60     0.971053   0.985892                                 \n",
      "    61     0.975288   0.980382                                 \n",
      "    62     0.944891   0.970283                                 \n",
      "    63     0.953744   0.955654                                 \n",
      "    64     0.916447   0.937807                                 \n",
      "    65     0.906813   0.920993                                 \n",
      "    66     0.868731   0.906062                                 \n",
      "    67     0.858415   0.894852                                 \n",
      "    68     0.844099   0.888223                                 \n",
      "    69     0.854508   0.890044                                 \n",
      "    70     0.968992   0.981104                                 \n",
      "    71     0.956445   0.97213                                  \n",
      "    72     0.959195   0.960118                                 \n",
      "    73     0.929526   0.948718                                 \n",
      "    74     0.910905   0.930257                                 \n",
      "    75     0.885452   0.916552                                 \n",
      "    76     0.866505   0.900844                                 \n",
      "    77     0.847793   0.889465                                 \n",
      "    78     0.851904   0.883882                                 \n",
      "    79     0.829723   0.882001                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.88200098]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 8, wds=1e-6, cycle_len=10, cycle_save_name='adam3_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe0506f85724061b6223c441e8ff235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                 \n",
      "    0      0.829702   0.881273  \n",
      "    1      0.951012   0.970356                                 \n",
      "    2      0.967439   0.971461                                 \n",
      "    3      0.947978   0.969625                                 \n",
      "    4      0.967008   0.96368                                  \n",
      "    5      0.930247   0.955776                                 \n",
      "    6      0.915778   0.954348                                 \n",
      "    7      0.931548   0.939423                                 \n",
      "    8      0.916852   0.930579                                 \n",
      "    9      0.90054    0.918163                                 \n",
      "    10     0.87822    0.910958                                 \n",
      "    11     0.89871    0.899078                                 \n",
      "    12     0.861625   0.891122                                 \n",
      "    13     0.857402   0.883632                                 \n",
      "    14     0.866496   0.875259                                 \n",
      "    15     0.834881   0.870143                                 \n",
      "    16     0.822038   0.87043                                  \n",
      "    17     0.844584   0.864208                                 \n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1480/1668 [02:56<00:22,  8.41it/s, loss=0.805]"
     ]
    }
   ],
   "source": [
    "learner.fit(3e-3, 1, wds=1e-6, cycle_len=20, cycle_save_name='adam3_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('adam3_20_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('adam3_20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modeling accuracy is generally measured using the metric *perplexity*, which is simply `exp()` of the loss function we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.3926824434624"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(4.165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play around with our language model a bit to check it seems to be working OK. First, let's create a short bit of text to 'prime' a set of predictions. We'll use our torchtext field to numericalize it so we can feed it to our language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_str(s): return TEXT.preprocess(TEXT.tokenize(s))\n",
    "def num_str(s): return TEXT.numericalize([proc_str(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=learner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's see if our model can generate a bit more text all by itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model(m, s, l=250):\n",
    "    t = num_str(s)\n",
    "    m[0].bs=1\n",
    "    m.eval()\n",
    "    m.reset()\n",
    "    res,*_ = m(t)\n",
    "    print('...', end='')\n",
    "\n",
    "    for i in range(l):\n",
    "        n=res[-1].topk(2)[1]\n",
    "        n = n[1] if n.data[0]==0 else n[0]\n",
    "        word = TEXT.vocab.itos[n.data[0]]\n",
    "        print(word, end=' ')\n",
    "        if word=='<eos>': break\n",
    "        res,*_ = m(n[0].unsqueeze(0))\n",
    "\n",
    "    m[0].bs=bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...k:maj |: (3 =g, =a, =b, | =c 2 =e > =c =g > =c =e > =c | =d > =c =b, > =c =d > =e =f > =d | =c 2 =e > =c =g > =c =e > =c | =d > =c =b, > =a, =g, 2 (3 =g, =a, =b, | =c 2 =e > =c =g > =c =e > =c | =d > =c =b, > =c =d > =e =f > =d | =g > =c =b > =a =g > =f =e > =d | =c 2 =c 2 =c 2 :| |: (3 =g =a =b | =c 2 =e > =c =g > =c =e > =c | =d > =c =b > =a =g > =f =e > =d | =c 2 =e > =c =g > =c =e > =c | =d > =c =b > =a =g > =f =e > =d | =c 2 =e > =c =g > =c =e > =c | =d > =c =b > =a =g > =f =e > =d | =c > =e =g > =c =a > =f =d > =b, | =c 2 =e 2 =c 2 :| m:4/4 k:maj |: (3 =g, =a, =b, | =c 2 =e > =c =g > =c =e > =c | =d > =c =b, > =c =d > =e =f > =d | =c 2 =e > =c =g > =c =e > =c | =d > "
     ]
    }
   ],
   "source": [
    "sample_model(m,\"M:4/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "123px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
